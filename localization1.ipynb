{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marakhi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "os.getcwd()\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils import to_categorical\n",
    "#import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "#from kt_utils import *\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width, height :  2745 2009 [0, 669, 1338, 2007] [0, 686, 1372, 2058, 2744]\n",
      "669 686\n",
      "669 1372\n",
      "669 2058\n",
      "669 2744\n",
      "1338 686\n",
      "1338 1372\n",
      "1338 2058\n",
      "1338 2744\n",
      "2007 686\n",
      "2007 1372\n",
      "2007 2058\n",
      "2007 2744\n",
      "width, height :  3089 2191 [0, 730, 1460, 2190] [0, 772, 1544, 2316, 3088]\n",
      "730 772\n",
      "730 1544\n",
      "730 2316\n",
      "730 3088\n",
      "1460 772\n",
      "1460 1544\n",
      "1460 2316\n",
      "1460 3088\n",
      "2190 772\n",
      "2190 1544\n",
      "2190 2316\n",
      "2190 3088\n",
      "width, height :  2761 2001 [0, 666, 1332, 1998] [0, 690, 1380, 2070, 2760]\n",
      "666 690\n",
      "666 1380\n",
      "666 2070\n",
      "666 2760\n",
      "1332 690\n",
      "1332 1380\n",
      "1332 2070\n",
      "1332 2760\n",
      "1998 690\n",
      "1998 1380\n",
      "1998 2070\n",
      "1998 2760\n",
      "width, height :  3089 2191 [0, 730, 1460, 2190] [0, 772, 1544, 2316, 3088]\n",
      "730 772\n",
      "730 1544\n",
      "730 2316\n",
      "730 3088\n",
      "1460 772\n",
      "1460 1544\n",
      "1460 2316\n",
      "1460 3088\n",
      "2190 772\n",
      "2190 1544\n",
      "2190 2316\n",
      "2190 3088\n",
      "width, height :  2769 1993 [0, 664, 1328, 1992] [0, 692, 1384, 2076, 2768]\n",
      "664 692\n",
      "664 1384\n",
      "664 2076\n",
      "664 2768\n",
      "1328 692\n",
      "1328 1384\n",
      "1328 2076\n",
      "1328 2768\n",
      "1992 692\n",
      "1992 1384\n",
      "1992 2076\n",
      "1992 2768\n",
      "width, height :  2641 1977 [0, 658, 1316, 1974] [0, 660, 1320, 1980, 2640]\n",
      "658 660\n",
      "658 1320\n",
      "658 1980\n",
      "658 2640\n",
      "1316 660\n",
      "1316 1320\n",
      "1316 1980\n",
      "1316 2640\n",
      "1974 660\n",
      "1974 1320\n",
      "1974 1980\n",
      "1974 2640\n",
      "width, height :  2792 2032 [0, 677, 1354, 2031] [0, 697, 1394, 2091, 2788]\n",
      "677 697\n",
      "677 1394\n",
      "677 2091\n",
      "677 2788\n",
      "1354 697\n",
      "1354 1394\n",
      "1354 2091\n",
      "1354 2788\n",
      "2031 697\n",
      "2031 1394\n",
      "2031 2091\n",
      "2031 2788\n",
      "width, height :  2641 1977 [0, 658, 1316, 1974] [0, 660, 1320, 1980, 2640]\n",
      "658 660\n",
      "658 1320\n",
      "658 1980\n",
      "658 2640\n",
      "1316 660\n",
      "1316 1320\n",
      "1316 1980\n",
      "1316 2640\n",
      "1974 660\n",
      "1974 1320\n",
      "1974 1980\n",
      "1974 2640\n",
      "width, height :  2753 1985 [0, 661, 1322, 1983] [0, 688, 1376, 2064, 2752]\n",
      "661 688\n",
      "661 1376\n",
      "661 2064\n",
      "661 2752\n",
      "1322 688\n",
      "1322 1376\n",
      "1322 2064\n",
      "1322 2752\n",
      "1983 688\n",
      "1983 1376\n",
      "1983 2064\n",
      "1983 2752\n",
      "width, height :  2873 2137 [0, 712, 1424, 2136] [0, 718, 1436, 2154, 2872]\n",
      "712 718\n",
      "712 1436\n",
      "712 2154\n",
      "712 2872\n",
      "1424 718\n",
      "1424 1436\n",
      "1424 2154\n",
      "1424 2872\n",
      "2136 718\n",
      "2136 1436\n",
      "2136 2154\n",
      "2136 2872\n",
      "width, height :  2769 2017 [0, 672, 1344, 2016] [0, 692, 1384, 2076, 2768]\n",
      "672 692\n",
      "672 1384\n",
      "672 2076\n",
      "672 2768\n",
      "1344 692\n",
      "1344 1384\n",
      "1344 2076\n",
      "1344 2768\n",
      "2016 692\n",
      "2016 1384\n",
      "2016 2076\n",
      "2016 2768\n",
      "width, height :  2873 2137 [0, 712, 1424, 2136] [0, 718, 1436, 2154, 2872]\n",
      "712 718\n",
      "712 1436\n",
      "712 2154\n",
      "712 2872\n",
      "1424 718\n",
      "1424 1436\n",
      "1424 2154\n",
      "1424 2872\n",
      "2136 718\n",
      "2136 1436\n",
      "2136 2154\n",
      "2136 2872\n",
      "width, height :  2657 1897 [0, 632, 1264, 1896] [0, 664, 1328, 1992, 2656]\n",
      "632 664\n",
      "632 1328\n",
      "632 1992\n",
      "632 2656\n",
      "1264 664\n",
      "1264 1328\n",
      "1264 1992\n",
      "1264 2656\n",
      "1896 664\n",
      "1896 1328\n",
      "1896 1992\n",
      "1896 2656\n",
      "width, height :  2705 1993 [0, 664, 1328, 1992] [0, 676, 1352, 2028, 2704]\n",
      "664 676\n",
      "664 1352\n",
      "664 2028\n",
      "664 2704\n",
      "1328 676\n",
      "1328 1352\n",
      "1328 2028\n",
      "1328 2704\n",
      "1992 676\n",
      "1992 1352\n",
      "1992 2028\n",
      "1992 2704\n",
      "width, height :  2713 1937 [0, 645, 1290, 1935] [0, 678, 1356, 2034, 2712]\n",
      "645 678\n",
      "645 1356\n",
      "645 2034\n",
      "645 2712\n",
      "1290 678\n",
      "1290 1356\n",
      "1290 2034\n",
      "1290 2712\n",
      "1935 678\n",
      "1935 1356\n",
      "1935 2034\n",
      "1935 2712\n",
      "width, height :  2705 1993 [0, 664, 1328, 1992] [0, 676, 1352, 2028, 2704]\n",
      "664 676\n",
      "664 1352\n",
      "664 2028\n",
      "664 2704\n",
      "1328 676\n",
      "1328 1352\n",
      "1328 2028\n",
      "1328 2704\n",
      "1992 676\n",
      "1992 1352\n",
      "1992 2028\n",
      "1992 2704\n",
      "width, height :  2849 2041 [0, 680, 1360, 2040] [0, 712, 1424, 2136, 2848]\n",
      "680 712\n",
      "680 1424\n",
      "680 2136\n",
      "680 2848\n",
      "1360 712\n",
      "1360 1424\n",
      "1360 2136\n",
      "1360 2848\n",
      "2040 712\n",
      "2040 1424\n",
      "2040 2136\n",
      "2040 2848\n",
      "width, height :  2697 2049 [0, 682, 1364, 2046] [0, 674, 1348, 2022, 2696]\n",
      "682 674\n",
      "682 1348\n",
      "682 2022\n",
      "682 2696\n",
      "1364 674\n",
      "1364 1348\n",
      "1364 2022\n",
      "1364 2696\n",
      "2046 674\n",
      "2046 1348\n",
      "2046 2022\n",
      "2046 2696\n",
      "width, height :  2721 1969 [0, 656, 1312, 1968] [0, 680, 1360, 2040, 2720]\n",
      "656 680\n",
      "656 1360\n",
      "656 2040\n",
      "656 2720\n",
      "1312 680\n",
      "1312 1360\n",
      "1312 2040\n",
      "1312 2720\n",
      "1968 680\n",
      "1968 1360\n",
      "1968 2040\n",
      "1968 2720\n",
      "width, height :  2697 2049 [0, 682, 1364, 2046] [0, 674, 1348, 2022, 2696]\n",
      "682 674\n",
      "682 1348\n",
      "682 2022\n",
      "682 2696\n",
      "1364 674\n",
      "1364 1348\n",
      "1364 2022\n",
      "1364 2696\n",
      "2046 674\n",
      "2046 1348\n",
      "2046 2022\n",
      "2046 2696\n",
      "width, height :  2689 1913 [0, 637, 1274, 1911] [0, 672, 1344, 2016, 2688]\n",
      "637 672\n",
      "637 1344\n",
      "637 2016\n",
      "637 2688\n",
      "1274 672\n",
      "1274 1344\n",
      "1274 2016\n",
      "1274 2688\n",
      "1911 672\n",
      "1911 1344\n",
      "1911 2016\n",
      "1911 2688\n",
      "width, height :  3113 2185 [0, 728, 1456, 2184] [0, 778, 1556, 2334, 3112]\n",
      "728 778\n",
      "728 1556\n",
      "728 2334\n",
      "728 3112\n",
      "1456 778\n",
      "1456 1556\n",
      "1456 2334\n",
      "1456 3112\n",
      "2184 778\n",
      "2184 1556\n",
      "2184 2334\n",
      "2184 3112\n",
      "width, height :  2985 2137 [0, 712, 1424, 2136] [0, 746, 1492, 2238, 2984]\n",
      "712 746\n",
      "712 1492\n",
      "712 2238\n",
      "712 2984\n",
      "1424 746\n",
      "1424 1492\n",
      "1424 2238\n",
      "1424 2984\n",
      "2136 746\n",
      "2136 1492\n",
      "2136 2238\n",
      "2136 2984\n",
      "width, height :  3113 2185 [0, 728, 1456, 2184] [0, 778, 1556, 2334, 3112]\n",
      "728 778\n",
      "728 1556\n",
      "728 2334\n",
      "728 3112\n",
      "1456 778\n",
      "1456 1556\n",
      "1456 2334\n",
      "1456 3112\n",
      "2184 778\n",
      "2184 1556\n",
      "2184 2334\n",
      "2184 3112\n",
      "width, height :  2801 2017 [0, 672, 1344, 2016] [0, 700, 1400, 2100, 2800]\n",
      "672 700\n",
      "672 1400\n",
      "672 2100\n",
      "672 2800\n",
      "1344 700\n",
      "1344 1400\n",
      "1344 2100\n",
      "1344 2800\n",
      "2016 700\n",
      "2016 1400\n",
      "2016 2100\n",
      "2016 2800\n",
      "width, height :  3113 2199 [0, 732, 1464, 2196] [0, 778, 1556, 2334, 3112]\n",
      "732 778\n",
      "732 1556\n",
      "732 2334\n",
      "732 3112\n",
      "1464 778\n",
      "1464 1556\n",
      "1464 2334\n",
      "1464 3112\n",
      "2196 778\n",
      "2196 1556\n",
      "2196 2334\n",
      "2196 3112\n",
      "width, height :  2953 2137 [0, 712, 1424, 2136] [0, 738, 1476, 2214, 2952]\n",
      "712 738\n",
      "712 1476\n",
      "712 2214\n",
      "712 2952\n",
      "1424 738\n",
      "1424 1476\n",
      "1424 2214\n",
      "1424 2952\n",
      "2136 738\n",
      "2136 1476\n",
      "2136 2214\n",
      "2136 2952\n",
      "width, height :  3113 2199 [0, 732, 1464, 2196] [0, 778, 1556, 2334, 3112]\n",
      "732 778\n",
      "732 1556\n",
      "732 2334\n",
      "732 3112\n",
      "1464 778\n",
      "1464 1556\n",
      "1464 2334\n",
      "1464 3112\n",
      "2196 778\n",
      "2196 1556\n",
      "2196 2334\n",
      "2196 3112\n",
      "width, height :  2697 1953 [0, 650, 1300, 1950] [0, 674, 1348, 2022, 2696]\n",
      "650 674\n",
      "650 1348\n",
      "650 2022\n",
      "650 2696\n",
      "1300 674\n",
      "1300 1348\n",
      "1300 2022\n",
      "1300 2696\n",
      "1950 674\n",
      "1950 1348\n",
      "1950 2022\n",
      "1950 2696\n",
      "width, height :  3169 2191 [0, 730, 1460, 2190] [0, 792, 1584, 2376, 3168]\n",
      "730 792\n",
      "730 1584\n",
      "730 2376\n",
      "730 3168\n",
      "1460 792\n",
      "1460 1584\n",
      "1460 2376\n",
      "1460 3168\n",
      "2190 792\n",
      "2190 1584\n",
      "2190 2376\n",
      "2190 3168\n",
      "width, height :  2585 1833 [0, 610, 1220, 1830] [0, 646, 1292, 1938, 2584]\n",
      "610 646\n",
      "610 1292\n",
      "610 1938\n",
      "610 2584\n",
      "1220 646\n",
      "1220 1292\n",
      "1220 1938\n",
      "1220 2584\n",
      "1830 646\n",
      "1830 1292\n",
      "1830 1938\n",
      "1830 2584\n",
      "width, height :  3169 2191 [0, 730, 1460, 2190] [0, 792, 1584, 2376, 3168]\n",
      "730 792\n",
      "730 1584\n",
      "730 2376\n",
      "730 3168\n",
      "1460 792\n",
      "1460 1584\n",
      "1460 2376\n",
      "1460 3168\n",
      "2190 792\n",
      "2190 1584\n",
      "2190 2376\n",
      "2190 3168\n",
      "width, height :  2785 1961 [0, 653, 1306, 1959] [0, 696, 1392, 2088, 2784]\n",
      "653 696\n",
      "653 1392\n",
      "653 2088\n",
      "653 2784\n",
      "1306 696\n",
      "1306 1392\n",
      "1306 2088\n",
      "1306 2784\n",
      "1959 696\n",
      "1959 1392\n",
      "1959 2088\n",
      "1959 2784\n",
      "width, height :  3217 2207 [0, 735, 1470, 2205] [0, 804, 1608, 2412, 3216]\n",
      "735 804\n",
      "735 1608\n",
      "735 2412\n",
      "735 3216\n",
      "1470 804\n",
      "1470 1608\n",
      "1470 2412\n",
      "1470 3216\n",
      "2205 804\n",
      "2205 1608\n",
      "2205 2412\n",
      "2205 3216\n",
      "width, height :  2609 1881 [0, 626, 1252, 1878] [0, 652, 1304, 1956, 2608]\n",
      "626 652\n",
      "626 1304\n",
      "626 1956\n",
      "626 2608\n",
      "1252 652\n",
      "1252 1304\n",
      "1252 1956\n",
      "1252 2608\n",
      "1878 652\n",
      "1878 1304\n",
      "1878 1956\n",
      "1878 2608\n",
      "width, height :  3281 2199 [0, 732, 1464, 2196] [0, 820, 1640, 2460, 3280]\n",
      "732 820\n",
      "732 1640\n",
      "732 2460\n",
      "732 3280\n",
      "1464 820\n",
      "1464 1640\n",
      "1464 2460\n",
      "1464 3280\n",
      "2196 820\n",
      "2196 1640\n",
      "2196 2460\n",
      "2196 3280\n",
      "width, height :  2721 1953 [0, 650, 1300, 1950] [0, 680, 1360, 2040, 2720]\n",
      "650 680\n",
      "650 1360\n",
      "650 2040\n",
      "650 2720\n",
      "1300 680\n",
      "1300 1360\n",
      "1300 2040\n",
      "1300 2720\n",
      "1950 680\n",
      "1950 1360\n",
      "1950 2040\n",
      "1950 2720\n",
      "width, height :  3233 2177 [0, 725, 1450, 2175] [0, 808, 1616, 2424, 3232]\n",
      "725 808\n",
      "725 1616\n",
      "725 2424\n",
      "725 3232\n",
      "1450 808\n",
      "1450 1616\n",
      "1450 2424\n",
      "1450 3232\n",
      "2175 808\n",
      "2175 1616\n",
      "2175 2424\n",
      "2175 3232\n"
     ]
    }
   ],
   "source": [
    "A = 0\n",
    "k = 0\n",
    "for filename in glob.glob('C:\\\\Users\\\\Marakhi\\\\Desktop\\\\Deep learning ajay bhammar\\\\Deep learning exercise\\\\Data localization1\\\\raw data\\\\*.JPG'):\n",
    "    im=cv2.imread(filename)\n",
    "    x = im.shape\n",
    "    height = list(range(x[0]))\n",
    "    height = height[0:x[0]:int((x[0]-1)/4)]\n",
    "    width = list(range(x[1]))\n",
    "    width = width[0:x[1]:int((x[1]-1)/3)]\n",
    "    print(\"width, height : \",x[0], x[1], width, height)\n",
    "    for i in range(3):\n",
    "        for j in range(4):\n",
    "            crop_img = im[height[j]:height[j+1], width[i]:width[i+1]]\n",
    "            resize_img = cv2.resize(crop_img, (40,40), interpolation = cv2.INTER_AREA)\n",
    "            file_name = 'C:\\\\Users\\\\Marakhi\\\\Desktop\\\\Deep learning ajay bhammar\\\\Deep learning exercise\\\\Data localization1\\\\data\\\\'+str(A)+'.jpg'\n",
    "            cv2.imwrite(file_name, resize_img)\n",
    "            if k==0:\n",
    "                x_train=np.expand_dims(resize_img, axis=0)\n",
    "                k = k+1\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train, np.expand_dims(resize_img, axis=0)), axis=0)               \n",
    "            print(width[i+1],height[j+1])\n",
    "            A = A+1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 40, 40, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16db1bf8320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train1 = x_train[0:408,...]\n",
    "print(x_train1.shape)\n",
    "plt.imshow(x_train1[399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = genfromtxt('C:\\\\Users\\\\Marakhi\\\\Desktop\\\\Deep learning ajay bhammar\\\\Deep learning exercise\\\\Data localization1\\\\y_train.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[396])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HappyModel(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    X = Conv2D(32, (3, 3), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = MaxPooling2D((2, 2), name='max_pool0')(X)\n",
    "    \n",
    "    X = Conv2D(64, (3, 3), strides = (1, 1), name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = MaxPooling2D((2, 2), name='max_pool1')(X)\n",
    "    \n",
    "    X = Conv2D(128, (3, 3), strides = (1, 1), name = 'conv2')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn2')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = MaxPooling2D((2, 2), name='max_pool2')(X)\n",
    "    \n",
    "    X = Conv2D(3, (3, 3), strides = (1, 1), name = 'conv3')(X)\n",
    "    X = MaxPooling2D((2, 2), name='max_pool3')(X)\n",
    "\n",
    "    Y = Flatten()(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = Y, name='HappyModel')\n",
    "    #model = Model(inputs = X_input, outputs = y2, name='HappyModel')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(y_true, y_pred):\n",
    "    class_loss = tf.reduce_mean((y_true[:,:1]-y_pred[:,:1])**2,)\n",
    "    cond = tf.equal(tf.squeeze(y_true[:,:1]),1.)\n",
    "    ind = tf.where(cond)\n",
    "    val1 = tf.squeeze(tf.gather_nd(y_true[:,1:2],ind))\n",
    "    val2 = tf.squeeze(tf.gather_nd(y_pred[:,1:2],ind))\n",
    "    X_loss = tf.reduce_mean((val1-val2)**2,)\n",
    "    val3 = tf.squeeze(tf.gather_nd(y_true[:,2:3],ind))\n",
    "    val4 = tf.squeeze(tf.gather_nd(y_pred[:,2:3],ind))\n",
    "    Y_loss = tf.reduce_mean((val3-val4)**2,)\n",
    "    loss = class_loss + 5*X_loss + 5*Y_loss\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0272\n",
      "Epoch 2/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0348\n",
      "Epoch 3/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0361\n",
      "Epoch 4/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0204\n",
      "Epoch 5/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0315\n",
      "Epoch 6/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0322\n",
      "Epoch 7/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0331\n",
      "Epoch 8/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0295\n",
      "Epoch 9/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0261\n",
      "Epoch 10/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0235\n",
      "Epoch 11/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0199\n",
      "Epoch 12/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0225\n",
      "Epoch 13/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0230\n",
      "Epoch 14/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0178\n",
      "Epoch 15/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0111\n",
      "Epoch 16/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0141\n",
      "Epoch 17/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0086\n",
      "Epoch 18/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0098\n",
      "Epoch 19/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0100\n",
      "Epoch 20/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0116\n",
      "Epoch 21/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0092\n",
      "Epoch 22/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0108\n",
      "Epoch 23/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0084\n",
      "Epoch 24/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0157\n",
      "Epoch 25/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0136\n",
      "Epoch 26/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0122\n",
      "Epoch 27/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0138\n",
      "Epoch 28/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0171\n",
      "Epoch 29/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0102\n",
      "Epoch 30/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0082\n",
      "Epoch 31/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0095\n",
      "Epoch 32/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0134\n",
      "Epoch 33/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0079\n",
      "Epoch 34/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0090\n",
      "Epoch 35/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0174\n",
      "Epoch 36/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0155\n",
      "Epoch 37/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0105\n",
      "Epoch 38/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0141\n",
      "Epoch 39/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0105\n",
      "Epoch 40/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0258\n",
      "Epoch 41/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0118\n",
      "Epoch 42/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0155\n",
      "Epoch 43/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0256\n",
      "Epoch 44/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0149\n",
      "Epoch 45/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0074\n",
      "Epoch 46/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0117\n",
      "Epoch 47/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0113\n",
      "Epoch 48/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0093\n",
      "Epoch 49/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0072\n",
      "Epoch 50/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0094\n",
      "Epoch 51/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0154\n",
      "Epoch 52/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0170\n",
      "Epoch 53/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0102\n",
      "Epoch 54/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0056\n",
      "Epoch 55/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0085\n",
      "Epoch 56/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0107\n",
      "Epoch 57/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0117\n",
      "Epoch 58/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0112\n",
      "Epoch 59/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0124\n",
      "Epoch 60/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0134\n",
      "Epoch 61/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0091\n",
      "Epoch 62/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0088\n",
      "Epoch 63/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0081\n",
      "Epoch 64/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0107\n",
      "Epoch 65/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0177\n",
      "Epoch 66/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0111\n",
      "Epoch 67/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0065\n",
      "Epoch 68/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0084\n",
      "Epoch 69/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0174\n",
      "Epoch 70/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0190\n",
      "Epoch 71/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0287\n",
      "Epoch 72/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0286\n",
      "Epoch 73/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0202\n",
      "Epoch 74/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0210\n",
      "Epoch 75/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0132\n",
      "Epoch 76/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0129\n",
      "Epoch 77/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0097\n",
      "Epoch 78/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0114\n",
      "Epoch 79/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0073\n",
      "Epoch 80/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0049\n",
      "Epoch 81/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0053\n",
      "Epoch 82/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0042\n",
      "Epoch 83/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0097\n",
      "Epoch 84/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0144\n",
      "Epoch 85/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0147\n",
      "Epoch 86/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0089\n",
      "Epoch 87/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0095\n",
      "Epoch 88/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0101\n",
      "Epoch 89/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0104\n",
      "Epoch 90/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0078\n",
      "Epoch 91/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0313\n",
      "Epoch 92/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0404\n",
      "Epoch 93/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0228\n",
      "Epoch 94/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0253\n",
      "Epoch 95/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0260\n",
      "Epoch 96/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0190\n",
      "Epoch 97/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0197\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0157\n",
      "Epoch 99/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0089\n",
      "Epoch 100/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0107\n",
      "Epoch 101/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0082\n",
      "Epoch 102/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0085\n",
      "Epoch 103/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0092\n",
      "Epoch 104/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0086\n",
      "Epoch 105/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0097\n",
      "Epoch 106/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0121\n",
      "Epoch 107/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0186\n",
      "Epoch 108/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0227\n",
      "Epoch 109/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0242\n",
      "Epoch 110/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0534\n",
      "Epoch 111/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0481\n",
      "Epoch 112/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0683\n",
      "Epoch 113/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0381\n",
      "Epoch 114/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0178\n",
      "Epoch 115/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0193\n",
      "Epoch 116/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0294\n",
      "Epoch 117/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0271\n",
      "Epoch 118/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0170\n",
      "Epoch 119/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0110\n",
      "Epoch 120/200\n",
      "408/408 [==============================] - 9s 21ms/step - loss: 0.0083\n",
      "Epoch 121/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0097\n",
      "Epoch 122/200\n",
      "408/408 [==============================] - 11s 26ms/step - loss: 0.0093\n",
      "Epoch 123/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0073\n",
      "Epoch 124/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0059\n",
      "Epoch 125/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0063\n",
      "Epoch 126/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0055\n",
      "Epoch 127/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0092\n",
      "Epoch 128/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0149\n",
      "Epoch 129/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0120\n",
      "Epoch 130/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0078\n",
      "Epoch 131/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0155\n",
      "Epoch 132/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0113\n",
      "Epoch 133/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0120\n",
      "Epoch 134/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0144\n",
      "Epoch 135/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0178\n",
      "Epoch 136/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0241\n",
      "Epoch 137/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0212\n",
      "Epoch 138/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0271\n",
      "Epoch 139/200\n",
      "408/408 [==============================] - 10s 25ms/step - loss: 0.0286\n",
      "Epoch 140/200\n",
      "408/408 [==============================] - 12s 28ms/step - loss: 0.0266\n",
      "Epoch 141/200\n",
      "408/408 [==============================] - 11s 27ms/step - loss: 0.0138\n",
      "Epoch 142/200\n",
      "408/408 [==============================] - 11s 28ms/step - loss: 0.0089\n",
      "Epoch 143/200\n",
      "408/408 [==============================] - 11s 27ms/step - loss: 0.0134\n",
      "Epoch 144/200\n",
      "408/408 [==============================] - 12s 28ms/step - loss: 0.0133\n",
      "Epoch 145/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0119\n",
      "Epoch 146/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0091\n",
      "Epoch 147/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0082\n",
      "Epoch 148/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0051\n",
      "Epoch 149/200\n",
      "408/408 [==============================] - 10s 23ms/step - loss: 0.0057\n",
      "Epoch 150/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0095\n",
      "Epoch 151/200\n",
      "408/408 [==============================] - 10s 25ms/step - loss: 0.0053\n",
      "Epoch 152/200\n",
      "408/408 [==============================] - 11s 26ms/step - loss: 0.0051\n",
      "Epoch 153/200\n",
      "408/408 [==============================] - 10s 25ms/step - loss: 0.0060\n",
      "Epoch 154/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0048\n",
      "Epoch 155/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0049\n",
      "Epoch 156/200\n",
      "408/408 [==============================] - 10s 23ms/step - loss: 0.0042\n",
      "Epoch 157/200\n",
      "408/408 [==============================] - 10s 26ms/step - loss: 0.0034\n",
      "Epoch 158/200\n",
      "408/408 [==============================] - 11s 26ms/step - loss: 0.0036\n",
      "Epoch 159/200\n",
      "408/408 [==============================] - 10s 25ms/step - loss: 0.0078\n",
      "Epoch 160/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0062\n",
      "Epoch 161/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0070\n",
      "Epoch 162/200\n",
      "408/408 [==============================] - 9s 22ms/step - loss: 0.0075\n",
      "Epoch 163/200\n",
      "408/408 [==============================] - 10s 25ms/step - loss: 0.0119\n",
      "Epoch 164/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0098\n",
      "Epoch 165/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0080\n",
      "Epoch 166/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0086\n",
      "Epoch 167/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0071\n",
      "Epoch 168/200\n",
      "408/408 [==============================] - 10s 26ms/step - loss: 0.0069\n",
      "Epoch 169/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0062\n",
      "Epoch 170/200\n",
      "408/408 [==============================] - 10s 25ms/step - loss: 0.0053\n",
      "Epoch 171/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0104\n",
      "Epoch 172/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0111\n",
      "Epoch 173/200\n",
      "408/408 [==============================] - 9s 23ms/step - loss: 0.0143\n",
      "Epoch 174/200\n",
      "408/408 [==============================] - 13s 31ms/step - loss: 0.0248\n",
      "Epoch 175/200\n",
      "408/408 [==============================] - 11s 27ms/step - loss: 0.0267\n",
      "Epoch 176/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0152\n",
      "Epoch 177/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0125\n",
      "Epoch 178/200\n",
      "408/408 [==============================] - 10s 25ms/step - loss: 0.0122\n",
      "Epoch 179/200\n",
      "408/408 [==============================] - 11s 27ms/step - loss: 0.0116\n",
      "Epoch 180/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0113\n",
      "Epoch 181/200\n",
      "408/408 [==============================] - 11s 26ms/step - loss: 0.0114\n",
      "Epoch 182/200\n",
      "408/408 [==============================] - 12s 29ms/step - loss: 0.0158\n",
      "Epoch 183/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0166\n",
      "Epoch 184/200\n",
      "408/408 [==============================] - 11s 26ms/step - loss: 0.0134\n",
      "Epoch 185/200\n",
      "408/408 [==============================] - 11s 27ms/step - loss: 0.0113\n",
      "Epoch 186/200\n",
      "408/408 [==============================] - 11s 27ms/step - loss: 0.0068\n",
      "Epoch 187/200\n",
      "408/408 [==============================] - 11s 26ms/step - loss: 0.0058\n",
      "Epoch 188/200\n",
      "408/408 [==============================] - 10s 25ms/step - loss: 0.0069\n",
      "Epoch 189/200\n",
      "408/408 [==============================] - 10s 25ms/step - loss: 0.0091\n",
      "Epoch 190/200\n",
      "408/408 [==============================] - 10s 26ms/step - loss: 0.0079\n",
      "Epoch 191/200\n",
      "408/408 [==============================] - 12s 30ms/step - loss: 0.0086\n",
      "Epoch 192/200\n",
      "408/408 [==============================] - 10s 25ms/step - loss: 0.0069\n",
      "Epoch 193/200\n",
      "408/408 [==============================] - 11s 28ms/step - loss: 0.0106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/200\n",
      "408/408 [==============================] - 11s 26ms/step - loss: 0.0057\n",
      "Epoch 195/200\n",
      "408/408 [==============================] - 11s 26ms/step - loss: 0.0053\n",
      "Epoch 196/200\n",
      "408/408 [==============================] - 12s 28ms/step - loss: 0.0066\n",
      "Epoch 197/200\n",
      "408/408 [==============================] - 11s 26ms/step - loss: 0.0101\n",
      "Epoch 198/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0077\n",
      "Epoch 199/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0076\n",
      "Epoch 200/200\n",
      "408/408 [==============================] - 10s 24ms/step - loss: 0.0089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16db87a3e80>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#happyModel = HappyModel(x_train1.shape[1:])\n",
    "#happyModel.compile('adam', loss=my_loss)\n",
    "happyModel.fit(x_train1, y_train, epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8146203  0.48836845 0.482537  ]]\n"
     ]
    }
   ],
   "source": [
    "x = image.img_to_array(x_train[408])\n",
    "x = np.expand_dims(x, axis=0)\n",
    "#x = preprocess_input(x)\n",
    "#print(y_train[0])\n",
    "mark_y=happyModel.predict(x)\n",
    "print(mark_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[ 0  3  6]\n",
      " [ 9 12 15]\n",
      " [18 21 24]]\n",
      "Tensor(\"Mean:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "XA = np.arange(9).reshape(3,3)\n",
    "XB = np.arange(9).reshape(3,3)*3\n",
    "print(XA)\n",
    "print(XB)\n",
    "XC = my_loss(XA,XB)\n",
    "print(XC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1]\n"
     ]
    }
   ],
   "source": [
    "val = 3\n",
    "m = tf.placeholder(tf.int32)\n",
    "m_feed = [[0  ,   0, val,   0, val],\n",
    "          [val,   0, val, val,   0],\n",
    "          [0  , val,   0,   0,   0]]\n",
    "\n",
    "tmp_indices = tf.where(tf.equal(m, val))\n",
    "result = tf.segment_min(tmp_indices[:, 1], tmp_indices[:, 0])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(result, feed_dict={m: m_feed})) # [2, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33333334\n",
      "1.625\n",
      "1.9583334\n"
     ]
    }
   ],
   "source": [
    "XA = tf.placeholder(\"float\", [None, 3])\n",
    "XB = tf.placeholder(\"float\", [None, 3])\n",
    "class_loss = tf.reduce_mean((XA[:,:1]-XB[:,:1])**2,)\n",
    "cond = tf.equal(tf.squeeze(XA[:,:1]),1.)\n",
    "ind = tf.where(cond)\n",
    "val1 = tf.squeeze(tf.gather_nd(XA[:,1:2],ind))\n",
    "val2 = tf.squeeze(tf.gather_nd(XB[:,1:2],ind))\n",
    "#ind = tf.squeeze(XA[:,:1])\n",
    "X_loss = tf.reduce_mean((val1-val2)**2,)\n",
    "XA1 = [[0, 1, 2],\n",
    " [1, 4.5, 5],\n",
    " [1, 7, 8]]\n",
    "\n",
    "XB1 = [[0, 1, 2],\n",
    " [1, 3, 5],\n",
    " [0, 8, 8]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(class_loss, feed_dict={XA: XA1, XB: XB1}))\n",
    "    print(sess.run(X_loss, feed_dict={XA: XA1, XB: XB1}))\n",
    "    print(sess.run(X_loss+class_loss, feed_dict={XA: XA1, XB: XB1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n",
      "[[0.9021167  0.480259   0.38550788]]\n",
      "409\n",
      "[[1.0037224  0.49977154 0.40895304]]\n",
      "410\n",
      "[[0.9219107  0.56019306 0.55963135]]\n",
      "411\n",
      "[[0.82205886 0.5832147  0.48235163]]\n",
      "412\n",
      "[[0.81097543 0.47251543 0.42124534]]\n",
      "413\n",
      "[[0.9399446 0.411659  0.3691232]]\n",
      "414\n",
      "[[0.9812643  0.63263714 0.6192992 ]]\n",
      "415\n",
      "[[0.99309725 0.6319423  0.462965  ]]\n",
      "416\n",
      "[[1.0541133  0.51542574 0.4770237 ]]\n",
      "417\n",
      "[[0.8655816  0.47659633 0.49604216]]\n",
      "418\n",
      "[[1.0456866  0.42949972 0.6704128 ]]\n",
      "419\n",
      "[[1.0317092  0.67154706 0.47585204]]\n",
      "420\n",
      "[[-0.06970196  0.4749639   0.3875537 ]]\n",
      "421\n",
      "[[-0.0780332   0.39501336  0.33676705]]\n",
      "422\n",
      "[[-0.06401201  0.399692    0.32831126]]\n",
      "423\n",
      "[[-0.03797341  0.38859758  0.39282563]]\n",
      "424\n",
      "[[-0.20504905  0.7242935   0.56774986]]\n",
      "425\n",
      "[[-0.14062183  0.6137549   0.5298977 ]]\n",
      "426\n",
      "[[-0.09976786  0.60900545  0.53028643]]\n",
      "427\n",
      "[[-0.07239731  0.6387845   0.65264523]]\n",
      "428\n",
      "[[-0.08007289  0.49777976  0.3951419 ]]\n",
      "429\n",
      "[[-0.01460615  0.41312104  0.38231406]]\n",
      "430\n",
      "[[-0.04773486  0.41529608  0.3775662 ]]\n",
      "431\n",
      "[[0.01368363 0.42315745 0.38937056]]\n",
      "432\n",
      "[[1.0202341  0.49190047 0.4316699 ]]\n",
      "433\n",
      "[[0.9023066  0.47894147 0.57486594]]\n",
      "434\n",
      "[[0.99302715 0.47119197 0.42442557]]\n",
      "435\n",
      "[[1.030898   0.5001207  0.48262724]]\n",
      "436\n",
      "[[0.9514358  0.345334   0.41089168]]\n",
      "437\n",
      "[[0.94944054 0.55126965 0.4517573 ]]\n",
      "438\n",
      "[[1.0735837  0.46116057 0.3849111 ]]\n",
      "439\n",
      "[[0.9818316 0.5416422 0.5333091]]\n",
      "440\n",
      "[[1.2099333  0.56076497 0.45747098]]\n",
      "441\n",
      "[[0.7695583  0.60589844 0.27460542]]\n",
      "442\n",
      "[[1.0319514  0.5161406  0.29018778]]\n",
      "443\n",
      "[[1.0624152 0.6279076 0.3771747]]\n",
      "444\n",
      "[[-0.07582878  0.4560936   0.39050698]]\n",
      "445\n",
      "[[-0.04937821  0.3954535   0.33086118]]\n",
      "446\n",
      "[[-0.03324148  0.3962667   0.3317233 ]]\n",
      "447\n",
      "[[-0.04016936  0.3687881   0.3640331 ]]\n",
      "448\n",
      "[[-0.12729701  0.64924014  0.5041148 ]]\n",
      "449\n",
      "[[-0.05539807  0.6092675   0.5070366 ]]\n",
      "450\n",
      "[[-0.03517615  0.5972496   0.5180497 ]]\n",
      "451\n",
      "[[-0.05738528  0.60785073  0.6371565 ]]\n",
      "452\n",
      "[[-0.05025991  0.46827963  0.40886664]]\n",
      "453\n",
      "[[0.08883224 0.40683666 0.37089142]]\n",
      "454\n",
      "[[-0.00918808  0.41067547  0.37354735]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(408,455,1):\n",
    "    file_name = 'C:\\\\Users\\\\Marakhi\\\\Desktop\\\\Deep learning ajay bhammar\\\\Deep learning exercise\\\\Data localization1\\\\Localization1\\\\'+str(i)+'.jpg'\n",
    "    im = cv2.imread(file_name)\n",
    "    print(i)\n",
    "    x = image.img_to_array(im)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    tar_y = happyModel.predict(x)\n",
    "    print(tar_y)\n",
    "    if tar_y[0][0]>0.5:\n",
    "        locliz = cv2.circle(im,(int(tar_y[0][1]*40),int(tar_y[0][2]*40)),5,(255,255,255))    \n",
    "        cv2.imwrite(file_name, locliz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
